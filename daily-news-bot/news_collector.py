#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Daily News Bot - Article Collector and Slack Poster
Collects Japanese articles from RSS feeds and posts to Slack
"""

import json
import hashlib
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import requests
import feedparser


class Article:
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    def __init__(self, title: str, url: str, summary: str, source: str, published: Optional[datetime] = None):
        self.title = title
        self.url = url
        self.summary = summary
        self.source = source
        self.published = published
        self.tags = self._extract_tags()
        self.score = 0.0

    def _extract_tags(self) -> List[str]:
        """è¨˜äº‹ã‹ã‚‰ã‚¿ã‚°ã‚’æŠ½å‡º"""
        tags = []
        text = (self.title + " " + self.summary).lower()

        tag_keywords = {
            "AI": ["ai", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ç”Ÿæˆai", "chatgpt"],
            "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°": ["ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "åºƒå‘Š", "ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³"],
            "SNS": ["sns", "twitter", "instagram", "tiktok", "facebook", "x"],
            "ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£": ["ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£", "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "ãƒ•ã‚¡ãƒ³"],
            "ãƒ‡ãƒ¼ã‚¿åˆ†æ": ["ãƒ‡ãƒ¼ã‚¿", "åˆ†æ", "èª¿æŸ»", "çµ±è¨ˆ"],
            "æˆ¦ç•¥": ["æˆ¦ç•¥", "æ–½ç­–", "æ‰‹æ³•"],
            "äº‹ä¾‹": ["äº‹ä¾‹", "ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£", "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"],
        }

        for tag, keywords in tag_keywords.items():
            if any(kw in text for kw in keywords):
                tags.append(tag)

        return tags if tags else ["ãã®ä»–"]

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "title": self.title,
            "url": self.url,
            "summary": self.summary,
            "source": self.source,
            "tags": self.tags,
            "published": self.published.isoformat() if self.published else None
        }


class NewsCollector:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¯ãƒ©ã‚¹"""

    RSS_FEEDS = {
        "æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰": [
            "https://xtrend.nikkei.com/rss/atcl/new.rdf",
            "https://xtrend.nikkei.com/rss/atcl/feature.rdf"
        ],
        "MarkeZine": [
            "https://markezine.jp/rss/new/20/index.xml"
        ],
        "ITmedia ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°": [
            "https://marketing.itmedia.co.jp/mm/rss/news.xml"
        ],
        "æ—¥çµŒãƒ“ã‚¸ãƒã‚¹": [
            "https://business.nikkei.com/rss/nb.rdf"
        ],
        "CNET Japan": [
            "https://japan.cnet.com/rss/index.rdf"
        ]
    }

    EXCLUDE_KEYWORDS = [
        "æœ‰æ–™ä¼šå“¡", "ä¼šå“¡é™å®š", "æœ‰æ–™è¨˜äº‹", "ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹", "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒªãƒ¼ã‚¹",
        "é–‹å‚¬ã®ãŠçŸ¥ã‚‰ã›", "ã‚¤ãƒ™ãƒ³ãƒˆé–‹å‚¬", "è¬›æ¼”ä¼š", "å‚åŠ è€…å‹Ÿé›†", "ç™ºå£²é–‹å§‹", "æ–°ç™ºå£²"
    ]

    EXCLUDE_DOMAINS = ["prtimes.jp", "atpress.ne.jp", "pr-today.net"]

    PRIORITY_KEYWORDS = [
        "åˆ†æ", "ãƒ‡ãƒ¼ã‚¿", "èª¿æŸ»", "ç ”ç©¶", "ãƒˆãƒ¬ãƒ³ãƒ‰",
        "æˆ¦ç•¥", "äº‹ä¾‹", "ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£", "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼",
        "è§£èª¬", "ãƒã‚¦ãƒã‚¦", "æ‰‹æ³•", "æœ€æ–°å‹•å‘"
    ]

    def __init__(self):
        self.sent_articles: set = set()
        self.learning_data: Dict = {}
        self.load_data()

    def load_data(self):
        """æ—¢èª­è¨˜äº‹ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open('sent_articles.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.sent_articles = set(data.get('articles', []))
        except FileNotFoundError:
            self.sent_articles = set()

        try:
            with open('learning_data.json', 'r', encoding='utf-8') as f:
                self.learning_data = json.load(f)
        except FileNotFoundError:
            self.learning_data = {"preferences": {"liked_sources": {}, "liked_tags": {}}}

    def save_sent_articles(self):
        """æ—¢èª­è¨˜äº‹ã‚’ä¿å­˜ï¼ˆæœ€æ–°1000ä»¶ï¼‰"""
        articles_list = list(self.sent_articles)[-1000:]
        with open('sent_articles.json', 'w', encoding='utf-8') as f:
            json.dump({"articles": articles_list, "last_updated": datetime.now().isoformat()}, f, ensure_ascii=False, indent=2)

    @staticmethod
    def clean_html(text: str) -> str:
        """HTMLã‚¿ã‚°ã‚’é™¤å»"""
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    @staticmethod
    def url_hash(url: str) -> str:
        """URLã‚’MD5ãƒãƒƒã‚·ãƒ¥åŒ–"""
        return hashlib.md5(url.encode()).hexdigest()

    def fetch_rss_feed(self, url: str, source: str) -> List[Article]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
        articles = []
        cutoff_date = datetime.now() - timedelta(days=3)

        try:
            print(f"  Fetching from {source}: {url}")
            feed = feedparser.parse(url)

            if not feed.entries:
                print(f"  Warning: No entries found in {source}")
                return articles

            for entry in feed.entries:
                try:
                    title = entry.get('title', '').strip()
                    link = entry.get('link', '').strip()
                    summary = entry.get('summary', '') or entry.get('description', '')
                    summary = self.clean_html(summary)

                    if not title or not link:
                        continue

                    # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    published = None
                    pub_parsed = entry.get('published_parsed') or entry.get('updated_parsed')
                    if pub_parsed:
                        try:
                            published = datetime(*pub_parsed[:6])
                            if published < cutoff_date:
                                continue
                        except (TypeError, ValueError):
                            pass

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if self.url_hash(link) in self.sent_articles:
                        continue

                    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    text = (title + " " + summary).lower()
                    if any(kw in text for kw in self.EXCLUDE_KEYWORDS):
                        continue

                    # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                    if any(domain in link for domain in self.EXCLUDE_DOMAINS):
                        continue

                    # è¦ç´„ã‚’200æ–‡å­—ä»¥å†…ã«åˆ¶é™
                    if len(summary) > 200:
                        summary = summary[:197] + "..."

                    article = Article(title, link, summary, source, published)
                    articles.append(article)

                except Exception as e:
                    print(f"  Error processing entry: {e}")
                    continue

            print(f"  Found {len(articles)} valid articles from {source}")

        except Exception as e:
            print(f"  Error fetching {source}: {e}")

        return articles

    def collect_articles(self) -> List[Article]:
        """å…¨RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’åé›†"""
        all_articles = []

        print("Collecting articles from RSS feeds...")
        for source, feed_urls in self.RSS_FEEDS.items():
            for feed_url in feed_urls:
                articles = self.fetch_rss_feed(feed_url, source)
                all_articles.extend(articles)
                time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

        # URLé‡è¤‡é™¤å»
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article.url not in seen_urls:
                seen_urls.add(article.url)
                unique_articles.append(article)

        print(f"Total unique articles collected: {len(unique_articles)}")
        return unique_articles

    def calculate_score(self, article: Article) -> float:
        """è¨˜äº‹ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0.0

        # æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æœ€å„ªå…ˆ
        if article.source == "æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰":
            score += 100.0

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹åŠ ç‚¹
        prefs = self.learning_data.get('preferences', {})
        liked_sources = prefs.get('liked_sources', {})
        if article.source in liked_sources:
            score += liked_sources[article.source] * 2.0

        liked_tags = prefs.get('liked_tags', {})
        for tag in article.tags:
            if tag in liked_tags:
                score += liked_tags[tag] * 1.5

        # å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹åŠ ç‚¹
        text = (article.title + " " + article.summary).lower()
        for keyword in self.PRIORITY_KEYWORDS:
            if keyword in text:
                score += 5.0

        return score

    def select_top_articles(self, articles: List[Article], count: int = 5) -> List[Article]:
        """ä¸Šä½è¨˜äº‹ã‚’é¸å®š"""
        for article in articles:
            article.score = self.calculate_score(article)

        sorted_articles = sorted(articles, key=lambda x: x.score, reverse=True)
        return sorted_articles[:count]


class SlackPoster:
    """SlackæŠ•ç¨¿ã‚¯ãƒ©ã‚¹"""

    NUMBER_EMOJIS = ["1ï¸âƒ£", "2ï¸âƒ£", "3ï¸âƒ£", "4ï¸âƒ£", "5ï¸âƒ£"]

    def __init__(self, bot_token: str, channel: str = "news"):
        self.bot_token = bot_token
        self.channel = channel
        self.base_url = "https://slack.com/api"

    def post_message(self, text: str, metadata: Optional[Dict] = None) -> Optional[str]:
        """Slackã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ•ç¨¿"""
        url = f"{self.base_url}/chat.postMessage"
        headers = {
            "Authorization": f"Bearer {self.bot_token}",
            "Content-Type": "application/json"
        }

        payload = {
            "channel": self.channel,
            "text": text,
            "unfurl_links": False,
            "unfurl_media": False

        }

        if metadata:
            payload["metadata"] = metadata

        try:
            response = requests.post(url, headers=headers, json=payload, timeout=10)
            response.raise_for_status()
            data = response.json()

            if data.get('ok'):
                return data.get('ts')
            else:
                print(f"Slack API error: {data.get('error')}")
                return None

        except Exception as e:
            print(f"Error posting message: {e}")
            return None

    def add_reaction(self, timestamp: str, emoji: str):
        """ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ """
        url = f"{self.base_url}/reactions.add"
        headers = {
            "Authorization": f"Bearer {self.bot_token}",
            "Content-Type": "application/json"
        }

        payload = {
            "channel": self.channel,
            "timestamp": timestamp,
            "name": emoji
        }

        try:
            response = requests.post(url, headers=headers, json=payload, timeout=10)
            response.raise_for_status()
            time.sleep(0.3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        except Exception as e:
            print(f"Error adding reaction: {e}")

    def post_daily_news(self, articles: List[Article]):
        """æ¯æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ•ç¨¿"""
        today = datetime.now().strftime("%Y-%m-%d")

        # ãƒ˜ãƒƒãƒ€ãƒ¼æŠ•ç¨¿ï¼ˆãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
        header = f"ğŸ“° ä»Šæ—¥ã®ãŠã™ã™ã‚è¨˜äº‹ ({today}) <@U05A1BUDW02>\nè‰¯ã‹ã£ãŸè¨˜äº‹ã«ã¯ğŸ‘ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã¤ã‘ã¦ãã ã•ã„ï¼"
        print(f"Posting header: {header}")
        self.post_message(header)
        time.sleep(1)

        # å„è¨˜äº‹ã‚’æŠ•ç¨¿
        for i, article in enumerate(articles):
            emoji_num = self.NUMBER_EMOJIS[i]
            tags_str = " ".join([f"#{tag}" for tag in article.tags])

            message = (
                f"{emoji_num} {article.title}\n"
                f"ğŸ‡¯ğŸ‡µ ğŸ”— {article.url}\n"
                f"ğŸ“ {article.summary}\n"
                f"ğŸ·ï¸ {tags_str} | ğŸ“° {article.source}"
            )

            metadata = {
                "event_type": "daily_news_article",
                "event_payload": {
                    "url": article.url,
                    "source": article.source,
                    "tags": article.tags
                }
            }

            print(f"Posting article {i+1}: {article.title[:50]}...")
            ts = self.post_message(message, metadata)

            if ts:
                self.add_reaction(ts, "thumbsup")
                self.add_reaction(ts, "thumbsdown")

            time.sleep(1)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import os

    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    slack_token = os.getenv('SLACK_BOT_TOKEN')
    if not slack_token:
        print("Error: SLACK_BOT_TOKEN not set")
        return

    print("=== Daily News Bot Started ===")
    print(f"Time: {datetime.now().isoformat()}")

    # è¨˜äº‹åé›†
    collector = NewsCollector()
    articles = collector.collect_articles()

    if not articles:
        print("No articles found. Exiting.")
        return

    # ä¸Šä½5è¨˜äº‹ã‚’é¸å®š
    top_articles = collector.select_top_articles(articles, count=5)

    print(f"\nSelected top {len(top_articles)} articles:")
    for i, article in enumerate(top_articles, 1):
        print(f"{i}. [{article.source}] {article.title} (score: {article.score:.2f})")

    # Slackã«æŠ•ç¨¿
    poster = SlackPoster(slack_token)
    poster.post_daily_news(top_articles)

    # æ—¢èª­è¨˜äº‹ã‚’æ›´æ–°
    for article in top_articles:
        collector.sent_articles.add(collector.url_hash(article.url))

    collector.save_sent_articles()

    print("\n=== Daily News Bot Completed ===")


if __name__ == "__main__":
    main()
