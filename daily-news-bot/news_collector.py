#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Daily News Collector for Slack
æ¯æ—¥æœ‰ç›Šãªè¨˜äº‹ã‚’åé›†ã—ã¦Slackã«æŠ•ç¨¿ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
import requests
from datetime import datetime, timedelta
from typing import List, Dict
import hashlib

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
SLACK_BOT_TOKEN = os.environ.get('SLACK_BOT_TOKEN')
SLACK_CHANNEL = 'news'
SLACK_USER_ID = 'U05A1BUDW02'
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')
GITHUB_REPO = os.environ.get('GITHUB_REPOSITORY')

# è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®å‹å®šç¾©
class Article:
    def __init__(self, title: str, url: str, summary: str, tags: List[str], source: str, lang: str):
        self.title = title
        self.url = url
        self.summary = summary
        self.tags = tags
        self.source = source
        self.lang = lang
        self.hash = hashlib.md5(url.encode()).hexdigest()

    def to_dict(self):
        return {
            'title': self.title,
            'url': self.url,
            'summary': self.summary,
            'tags': self.tags,
            'source': self.source,
            'lang': self.lang,
            'hash': self.hash
        }


def load_sent_articles() -> set:
    """æ—¢ã«é€ä¿¡æ¸ˆã¿ã®è¨˜äº‹ãƒãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
    try:
        # GitHub APIã‹ã‚‰æ—¢èª­ãƒªã‚¹ãƒˆã‚’å–å¾—
        headers = {
            'Authorization': f'token {GITHUB_TOKEN}',
            'Accept': 'application/vnd.github.v3+json'
        }
        url = f'https://api.github.com/repos/{GITHUB_REPO}/contents/daily-news-bot/sent_articles.json'
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            content = response.json()['content']
            import base64
            decoded = base64.b64decode(content).decode('utf-8')
            data = json.loads(decoded)
            return set(data.get('hashes', []))
        else:
            return set()
    except Exception as e:
        print(f"Error loading sent articles: {e}")
        return set()


def save_sent_articles(article_hashes: List[str]):
    """é€ä¿¡æ¸ˆã¿è¨˜äº‹ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
    try:
        sent_articles = load_sent_articles()
        sent_articles.update(article_hashes)

        # æœ€æ–°1000ä»¶ã®ã¿ä¿æŒï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        if len(sent_articles) > 1000:
            sent_articles = set(list(sent_articles)[-1000:])

        data = {
            'hashes': list(sent_articles),
            'last_updated': datetime.now().isoformat()
        }

        # GitHub APIã§ä¿å­˜
        headers = {
            'Authorization': f'token {GITHUB_TOKEN}',
            'Accept': 'application/vnd.github.v3+json'
        }
        url = f'https://api.github.com/repos/{GITHUB_REPO}/contents/daily-news-bot/sent_articles.json'

        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®SHAã‚’å–å¾—
        response = requests.get(url, headers=headers)
        sha = response.json()['sha'] if response.status_code == 200 else None

        import base64
        content = base64.b64encode(json.dumps(data, ensure_ascii=False, indent=2).encode()).decode()

        payload = {
            'message': f'Update sent articles {datetime.now().strftime("%Y-%m-%d")}',
            'content': content,
        }
        if sha:
            payload['sha'] = sha

        requests.put(url, headers=headers, json=payload)
        print(f"Saved {len(article_hashes)} new article hashes")

    except Exception as e:
        print(f"Error saving sent articles: {e}")


def search_nikkei_xtrend(keywords: List[str], days: int = 2) -> List[Article]:
    """æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’æ¤œç´¢ï¼ˆãƒ€ãƒŸãƒ¼å®Ÿè£…ï¼‰"""
    # æ³¨: å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯Robot.txtã¨APIã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
    # ã“ã“ã§ã¯æ§‹é€ ã®ã¿æä¾›
    articles = []
    print("æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰ã®è¨˜äº‹å–å¾—ã¯æ‰‹å‹•å®Ÿè£…ãŒå¿…è¦ã§ã™")
    return articles


def search_web_articles(keywords: List[str], days: int = 2, max_results: int = 20) -> List[Article]:
    """Webæ¤œç´¢APIã‚’ä½¿ã£ã¦è¨˜äº‹ã‚’æ¤œç´¢"""
    articles = []

    NEWS_API_KEY = os.environ.get('NEWS_API_KEY')

    if not NEWS_API_KEY:
        print("Warning: NEWS_API_KEY not set")
        return articles

    try:
        from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        url = 'https://newsapi.org/v2/everything'

        # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ï¼ˆè¨€èªæŒ‡å®šãªã—ã§å¹…åºƒãå–å¾—ï¼‰
        ja_keywords = ['ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'SNSãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'AI ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ãƒ‡ã‚¸ã‚¿ãƒ«ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°']

        for keyword in ja_keywords:
            params = {
                'q': keyword,
                'from': from_date,
                'sortBy': 'publishedAt',  # æœ€æ–°é †
                'pageSize': 10,
                'apiKey': NEWS_API_KEY
            }

            response = requests.get(url, params=params)
            if response.status_code == 200:
                data = response.json()
                for item in data.get('articles', []):
                    title = item.get('title', '')
                    description = item.get('description', '')

                    # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š
                    is_japanese = any('\u3040' <= char <= '\u30ff' or '\u4e00' <= char <= '\u9faf'
                                     for char in (title + description))

                    article = Article(
                        title=title,
                        url=item.get('url', ''),
                        summary=(description or '')[:200] + ('...' if description and len(description) > 200 else ''),
                        tags=[keyword],
                        source=item.get('source', {}).get('name', 'Unknown'),
                        lang='ja' if is_japanese else 'en'
                    )
                    articles.append(article)

        # è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚‚æ¤œç´¢ï¼ˆå°‘é‡ï¼‰
        en_keywords = ['community marketing', 'influencer marketing', 'AI marketing']

        for keyword in en_keywords[:2]:
            params = {
                'q': keyword,
                'language': 'en',
                'from': from_date,
                'sortBy': 'relevancy',
                'pageSize': 5,
                'apiKey': NEWS_API_KEY
            }

            response = requests.get(url, params=params)
            if response.status_code == 200:
                data = response.json()
                for item in data.get('articles', []):
                    article = Article(
                        title=item.get('title', ''),
                        url=item.get('url', ''),
                        summary=(item.get('description', '') or '')[:200] + '...',
                        tags=[keyword],
                        source=item.get('source', {}).get('name', 'Unknown'),
                        lang='en'
                    )
                    articles.append(article)

    except Exception as e:
        print(f"Error searching web articles: {e}")

    return articles


def load_learning_data() -> Dict:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        headers = {
            'Authorization': f'token {GITHUB_TOKEN}',
            'Accept': 'application/vnd.github.v3+json'
        }
        url = f'https://api.github.com/repos/{GITHUB_REPO}/contents/daily-news-bot/learning_data.json'
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            content = response.json()['content']
            import base64
            decoded = base64.b64decode(content).decode('utf-8')
            return json.loads(decoded)
        else:
            return {"preferences": {"liked_sources": {}, "liked_tags": {}, "liked_articles": []}}
    except Exception as e:
        print(f"Error loading learning data: {e}")
        return {"preferences": {"liked_sources": {}, "liked_tags": {}, "liked_articles": []}}


def calculate_article_score(article: Article, learning_data: Dict) -> float:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«è¨˜äº‹ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = 0.0
    prefs = learning_data.get('preferences', {})

    # ã‚½ãƒ¼ã‚¹ã®å¥½ã¿ã‚¹ã‚³ã‚¢
    liked_sources = prefs.get('liked_sources', {})
    if article.source in liked_sources:
        score += liked_sources[article.source] * 2.0

    # ã‚¿ã‚°ã®å¥½ã¿ã‚¹ã‚³ã‚¢
    liked_tags = prefs.get('liked_tags', {})
    for tag in article.tags:
        if tag in liked_tags:
            score += liked_tags[tag] * 1.5

    return score


def filter_and_rank_articles(articles: List[Article], sent_hashes: set, target_count: int = 5) -> List[Article]:
    """è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¦ä¸Šä½ã‚’è¿”ã™"""
    # æ—¢èª­ã‚’é™¤å¤–
    filtered = [a for a in articles if a.hash not in sent_hashes]

    # é‡è¤‡URLé™¤å¤–
    seen_urls = set()
    unique_articles = []
    for article in filtered:
        if article.url not in seen_urls:
            seen_urls.add(article.url)
            unique_articles.append(article)

    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¹ã‚³ã‚¢è¨ˆç®—
    learning_data = load_learning_data()

    # æ—¥æœ¬èªã¨è‹±èªã«åˆ†ã‘ã‚‹
    ja_articles = [a for a in unique_articles if a.lang == 'ja']
    en_articles = [a for a in unique_articles if a.lang == 'en']

    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆæ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰å„ªå…ˆ + å­¦ç¿’ã‚¹ã‚³ã‚¢ï¼‰
    ja_articles.sort(key=lambda x: (
        x.source != 'æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰',
        -calculate_article_score(x, learning_data)
    ))

    en_articles.sort(key=lambda x: -calculate_article_score(x, learning_data))

    # æ—¥æœ¬èª3è¨˜äº‹ + è‹±èª2è¨˜äº‹ã‚’é¸å®š
    ja_count = min(max(3, target_count - 2), len(ja_articles))  # æœ€ä½3è¨˜äº‹
    en_count = min(target_count - ja_count, len(en_articles))

    selected = ja_articles[:ja_count] + en_articles[:en_count]

    # è¶³ã‚Šãªã„å ´åˆã¯æ®‹ã‚Šã‚’è¿½åŠ 
    if len(selected) < target_count:
        remaining = [a for a in unique_articles if a not in selected]
        remaining.sort(key=lambda x: -calculate_article_score(x, learning_data))
        selected.extend(remaining[:target_count - len(selected)])

    return selected[:target_count]


def format_slack_message(articles: List[Article]) -> str:
    """SlackæŠ•ç¨¿ç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ•´å½¢"""
    today = datetime.now().strftime('%Y-%m-%d')

    message = f"ğŸ“° *ä»Šæ—¥ã®ãŠã™ã™ã‚è¨˜äº‹* ({today})\n<@{SLACK_USER_ID}>\n\n"

    emojis = ['1ï¸âƒ£', '2ï¸âƒ£', '3ï¸âƒ£', '4ï¸âƒ£', '5ï¸âƒ£']

    for idx, article in enumerate(articles):
        emoji = emojis[idx] if idx < len(emojis) else f"{idx+1}."
        lang_flag = 'ğŸ‡¯ğŸ‡µ' if article.lang == 'ja' else 'ğŸ‡ºğŸ‡¸'

        message += f"{emoji} *{article.title}* {lang_flag}\n"
        message += f"ğŸ”— {article.url}\n"
        message += f"ğŸ“ {article.summary}\n"
        message += f"ğŸ·ï¸ {' '.join(['#' + tag for tag in article.tags])} | ğŸ“° {article.source}\n\n"

    return message


def post_to_slack(message: str, articles: List[Article] = None) -> str:
    """Slackã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ•ç¨¿ã—ã€è¨˜äº‹æƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜"""
    url = 'https://slack.com/api/chat.postMessage'
    headers = {
        'Authorization': f'Bearer {SLACK_BOT_TOKEN}',
        'Content-Type': 'application/json'
    }

    # è¨˜äº‹æƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
    metadata = None
    if articles:
        metadata = {
            'event_type': 'daily_news',
            'event_payload': {
                'articles': [a.to_dict() for a in articles]
            }
        }

    payload = {
        'channel': SLACK_CHANNEL,
        'text': message,
        'unfurl_links': False,
        'unfurl_media': False
    }

    if metadata:
        payload['metadata'] = metadata

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code == 200 and response.json().get('ok'):
        print("âœ… Successfully posted to Slack")
        return response.json().get('ts', '')
    else:
        print(f"âŒ Failed to post to Slack: {response.text}")
        return ''


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== Daily News Collector Started ===")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©
    keywords = [
        'ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°',
        'SNSãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°',
        'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°',
        'AI',
        'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°',
        'community marketing',
        'influencer marketing'
    ]

    # æ—¢èª­è¨˜äº‹ã®èª­ã¿è¾¼ã¿
    sent_hashes = load_sent_articles()
    print(f"Loaded {len(sent_hashes)} sent article hashes")

    # è¨˜äº‹åé›†
    all_articles = []

    # Webæ¤œç´¢
    web_articles = search_web_articles(keywords, days=2, max_results=30)
    all_articles.extend(web_articles)
    print(f"Found {len(web_articles)} articles from web search")

    # æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ‰‹å‹•å®Ÿè£…ãŒå¿…è¦ï¼‰
    # nikkei_articles = search_nikkei_xtrend(keywords, days=2)
    # all_articles.extend(nikkei_articles)

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    selected_articles = filter_and_rank_articles(all_articles, sent_hashes, target_count=5)
    print(f"Selected {len(selected_articles)} articles to send")

    if len(selected_articles) == 0:
        print("âš ï¸ No new articles found")
        # è¨˜äº‹ãŒãªã„å ´åˆã‚‚Slackã«é€šçŸ¥
        message = f"ğŸ“° *ä»Šæ—¥ã®ãŠã™ã™ã‚è¨˜äº‹* ({datetime.now().strftime('%Y-%m-%d')})\n"
        message += f"<@{SLACK_USER_ID}>\n\n"
        message += "ä»Šæ—¥ã¯æ–°ã—ã„è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        post_to_slack(message)
        return

    # Slackã«æŠ•ç¨¿
    message = format_slack_message(selected_articles)
    message_ts = post_to_slack(message, selected_articles)

    # æ—¢èª­ãƒªã‚¹ãƒˆã«ä¿å­˜
    article_hashes = [a.hash for a in selected_articles]
    save_sent_articles(article_hashes)

    print(f"Message timestamp: {message_ts}")
    print("=== Daily News Collector Finished ===")


if __name__ == '__main__':
    main()
