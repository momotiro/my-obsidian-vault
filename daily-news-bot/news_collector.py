#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Daily News Collector for Slack
æ¯æ—¥æœ‰ç›Šãªè¨˜äº‹ã‚’åé›†ã—ã¦Slackã«æŠ•ç¨¿ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
import requests
import feedparser
import re
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional

# ç’°å¢ƒå¤‰æ•°
SLACK_BOT_TOKEN = os.environ.get('SLACK_BOT_TOKEN')
SLACK_CHANNEL = 'news'
SLACK_USER_ID = 'U05A1BUDW02'
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')
GITHUB_REPO = os.environ.get('GITHUB_REPOSITORY')
NEWS_API_KEY = os.environ.get('NEWS_API_KEY')


class Article:
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    def __init__(self, title: str, url: str, summary: str, tags: List[str], source: str, lang: str):
        self.title = title
        self.url = url
        self.summary = summary
        self.tags = tags
        self.source = source
        self.lang = lang
        self.hash = hashlib.md5(url.encode()).hexdigest()

    def to_dict(self):
        return {
            'title': self.title,
            'url': self.url,
            'summary': self.summary,
            'tags': self.tags,
            'source': self.source,
            'lang': self.lang,
            'hash': self.hash
        }


def github_get_file(file_path: str) -> Optional[Dict]:
    """GitHubã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
    try:
        headers = {
            'Authorization': f'token {GITHUB_TOKEN}',
            'Accept': 'application/vnd.github.v3+json'
        }
        url = f'https://api.github.com/repos/{GITHUB_REPO}/contents/{file_path}'
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code == 200:
            import base64
            content = response.json()['content']
            decoded = base64.b64decode(content).decode('utf-8')
            return {
                'data': json.loads(decoded),
                'sha': response.json()['sha']
            }
        return None
    except Exception as e:
        print(f"Error getting file from GitHub: {e}")
        return None


def github_save_file(file_path: str, data: Dict, sha: Optional[str] = None):
    """GitHubã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    try:
        headers = {
            'Authorization': f'token {GITHUB_TOKEN}',
            'Accept': 'application/vnd.github.v3+json'
        }
        url = f'https://api.github.com/repos/{GITHUB_REPO}/contents/{file_path}'

        import base64
        content = base64.b64encode(json.dumps(data, ensure_ascii=False, indent=2).encode()).decode()

        payload = {
            'message': f'Update {file_path} - {datetime.now().strftime("%Y-%m-%d %H:%M")}',
            'content': content,
        }
        if sha:
            payload['sha'] = sha

        response = requests.put(url, headers=headers, json=payload, timeout=10)
        if response.status_code in [200, 201]:
            print(f"âœ… Saved {file_path}")
            return True
        else:
            print(f"âŒ Failed to save {file_path}: {response.text}")
            return False
    except Exception as e:
        print(f"Error saving file to GitHub: {e}")
        return False


def load_sent_articles() -> set:
    """æ—¢èª­è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿"""
    file_data = github_get_file('daily-news-bot/sent_articles.json')
    if file_data:
        return set(file_data['data'].get('hashes', []))
    return set()


def save_sent_articles(article_hashes: List[str]):
    """æ—¢èª­è¨˜äº‹ã‚’ä¿å­˜"""
    try:
        file_data = github_get_file('daily-news-bot/sent_articles.json')
        existing_hashes = set(file_data['data'].get('hashes', [])) if file_data else set()
        existing_hashes.update(article_hashes)

        # æœ€æ–°1000ä»¶ã®ã¿ä¿æŒ
        if len(existing_hashes) > 1000:
            existing_hashes = set(list(existing_hashes)[-1000:])

        data = {
            'hashes': list(existing_hashes),
            'last_updated': datetime.now().isoformat()
        }

        sha = file_data['sha'] if file_data else None
        github_save_file('daily-news-bot/sent_articles.json', data, sha)
    except Exception as e:
        print(f"Error saving sent articles: {e}")


def load_learning_data() -> Dict:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    file_data = github_get_file('daily-news-bot/learning_data.json')
    if file_data:
        return file_data['data']
    return {"preferences": {"liked_sources": {}, "liked_tags": {}, "liked_articles": []}}


def search_nikkei_xtrend(days: int = 3) -> List[Article]:
    """æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰RSSã‹ã‚‰è¨˜äº‹å–å¾—"""
    articles = []

    rss_urls = [
        'https://xtrend.nikkei.com/rss/atcl/new.rdf',
        'https://xtrend.nikkei.com/rss/atcl/feature.rdf',
    ]

    cutoff_date = datetime.now() - timedelta(days=days)

    for rss_url in rss_urls:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:15]:
                published = entry.get('published_parsed')
                if published:
                    pub_date = datetime(*published[:6])
                    if pub_date < cutoff_date:
                        continue

                title = entry.get('title', '')
                url = entry.get('link', '')
                summary = entry.get('summary', '')
                summary = re.sub(r'<[^>]+>', '', summary)

                article = Article(
                    title=title,
                    url=url,
                    summary=summary[:200] + ('...' if len(summary) > 200 else ''),
                    tags=['ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°'],
                    source='æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰',
                    lang='ja'
                )
                articles.append(article)
        except Exception as e:
            print(f"Error fetching Nikkei RSS: {e}")
            continue

    print(f"Found {len(articles)} articles from Nikkei X-Trend")
    return articles


def search_web_articles(days: int = 2) -> List[Article]:
    """News APIã§è¨˜äº‹æ¤œç´¢"""
    articles = []

    if not NEWS_API_KEY:
        print("Warning: NEWS_API_KEY not set")
        return articles

    try:
        from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        url = 'https://newsapi.org/v2/everything'

        # æ—¥æœ¬èªè¨˜äº‹ã‚’æ¤œç´¢
        ja_queries = [
            'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚° (ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ OR SNS OR ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼)',
            'ãƒ‡ã‚¸ã‚¿ãƒ«ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚° AI',
        ]

        for query in ja_queries:
            params = {
                'q': query,
                'from': from_date,
                'language': 'ja',
                'sortBy': 'publishedAt',
                'pageSize': 20,
                'apiKey': NEWS_API_KEY
            }

            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for item in data.get('articles', []):
                    title = item.get('title', '')
                    description = item.get('description', '') or ''

                    # æ—¥æœ¬èªãƒã‚§ãƒƒã‚¯
                    is_japanese = any('\u3040' <= c <= '\u30ff' or '\u4e00' <= c <= '\u9faf'
                                     for c in (title + description))

                    if is_japanese:
                        article = Article(
                            title=title,
                            url=item.get('url', ''),
                            summary=description[:200] + ('...' if len(description) > 200 else ''),
                            tags=['ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°'],
                            source=item.get('source', {}).get('name', 'Unknown'),
                            lang='ja'
                        )
                        articles.append(article)

        # è‹±èªè¨˜äº‹ã‚‚å°‘é‡å–å¾—
        en_queries = ['community marketing', 'influencer marketing strategy']

        for query in en_queries[:1]:
            params = {
                'q': query,
                'from': from_date,
                'language': 'en',
                'sortBy': 'relevancy',
                'pageSize': 10,
                'apiKey': NEWS_API_KEY
            }

            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for item in data.get('articles', [])[:5]:
                    article = Article(
                        title=item.get('title', ''),
                        url=item.get('url', ''),
                        summary=(item.get('description', '') or '')[:200] + '...',
                        tags=['marketing'],
                        source=item.get('source', {}).get('name', 'Unknown'),
                        lang='en'
                    )
                    articles.append(article)

    except Exception as e:
        print(f"Error searching web articles: {e}")

    print(f"Found {len(articles)} articles from News API")
    return articles


def is_valuable_article(article: Article) -> bool:
    """æœ‰ç›Šãªè¨˜äº‹ã‹ãƒã‚§ãƒƒã‚¯"""
    title_lower = article.title.lower()
    summary_lower = (article.summary or '').lower()
    url_lower = article.url.lower()

    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    exclude_keywords = [
        'ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹', 'ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒªãƒ¼ã‚¹', 'press release',
        'é–‹å‚¬ã®ãŠçŸ¥ã‚‰ã›', 'ã‚¤ãƒ™ãƒ³ãƒˆé–‹å‚¬', 'è¬›æ¼”ä¼š', 'å‹Ÿé›†é–‹å§‹', 'å¿œå‹Ÿå—ä»˜',
        'æœ‰æ–™ä¼šå“¡', 'æœ‰æ–™è¨˜äº‹', 'ä¼šå“¡é™å®š', 'subscription required',
        '/release/', '/pr/', 'prtimes.jp', 'atpress.ne.jp'
    ]

    for keyword in exclude_keywords:
        if keyword in title_lower or keyword in summary_lower or keyword in url_lower:
            return False

    # æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å¸¸ã«OK
    if article.source == 'æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰':
        return True

    # æœ‰ç›Šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    valuable_keywords = [
        'åˆ†æ', 'ãƒ‡ãƒ¼ã‚¿', 'èª¿æŸ»', 'æˆ¦ç•¥', 'äº‹ä¾‹', 'ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼', 'è§£èª¬', 'ãƒã‚¦ãƒã‚¦',
        'analysis', 'data', 'strategy', 'case study', 'interview'
    ]

    has_valuable = any(k in title_lower or k in summary_lower for k in valuable_keywords)
    return has_valuable or len(article.title) > 20


def calculate_article_score(article: Article, learning_data: Dict) -> float:
    """è¨˜äº‹ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = 0.0
    prefs = learning_data.get('preferences', {})

    liked_sources = prefs.get('liked_sources', {})
    if article.source in liked_sources:
        score += liked_sources[article.source] * 2.0

    liked_tags = prefs.get('liked_tags', {})
    for tag in article.tags:
        if tag in liked_tags:
            score += liked_tags[tag] * 1.5

    return score


def filter_and_rank_articles(articles: List[Article], sent_hashes: set, learning_data: Dict) -> List[Article]:
    """è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
    # æ—¢èª­ãƒ»é™¤å¤–ãƒã‚§ãƒƒã‚¯
    filtered = [a for a in articles if a.hash not in sent_hashes and is_valuable_article(a)]

    # é‡è¤‡URLé™¤å¤–
    seen_urls = set()
    unique = []
    for a in filtered:
        if a.url not in seen_urls:
            seen_urls.add(a.url)
            unique.append(a)

    # æ—¥æœ¬èªãƒ»è‹±èªã«åˆ†é¡
    ja_articles = [a for a in unique if a.lang == 'ja']
    en_articles = [a for a in unique if a.lang == 'en']

    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    ja_articles.sort(key=lambda x: (
        x.source != 'æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰',
        -calculate_article_score(x, learning_data)
    ))
    en_articles.sort(key=lambda x: -calculate_article_score(x, learning_data))

    # æ—¥æœ¬èª3ä»¶ä»¥ä¸Šã€è‹±èª2ä»¶ä»¥ä¸‹
    ja_count = min(max(3, 5 - len(en_articles)), len(ja_articles))
    en_count = min(5 - ja_count, len(en_articles))

    selected = ja_articles[:ja_count] + en_articles[:en_count]

    # ä¸è¶³åˆ†ã‚’è£œå®Œ
    if len(selected) < 5:
        remaining = [a for a in unique if a not in selected]
        remaining.sort(key=lambda x: -calculate_article_score(x, learning_data))
        selected.extend(remaining[:5 - len(selected)])

    return selected[:5]


def post_to_slack(message: str, article: Optional[Article] = None) -> str:
    """Slackã«æŠ•ç¨¿"""
    url = 'https://slack.com/api/chat.postMessage'
    headers = {
        'Authorization': f'Bearer {SLACK_BOT_TOKEN}',
        'Content-Type': 'application/json'
    }

    payload = {
        'channel': SLACK_CHANNEL,
        'text': message,
        'unfurl_links': False,
        'unfurl_media': False
    }

    if article:
        payload['metadata'] = {
            'event_type': 'daily_news_article',
            'event_payload': {'article': article.to_dict()}
        }

    response = requests.post(url, headers=headers, json=payload, timeout=10)

    if response.status_code == 200 and response.json().get('ok'):
        return response.json().get('ts', '')
    else:
        print(f"âŒ Failed to post: {response.text}")
        return ''


def add_reaction(timestamp: str, emoji: str):
    """ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ """
    url = 'https://slack.com/api/reactions.add'
    headers = {
        'Authorization': f'Bearer {SLACK_BOT_TOKEN}',
        'Content-Type': 'application/json'
    }

    payload = {
        'channel': SLACK_CHANNEL,
        'timestamp': timestamp,
        'name': emoji
    }

    requests.post(url, headers=headers, json=payload, timeout=10)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== Daily News Collector Started ===")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    sent_hashes = load_sent_articles()
    learning_data = load_learning_data()
    print(f"Loaded {len(sent_hashes)} sent articles")

    # è¨˜äº‹åé›†
    all_articles = []
    all_articles.extend(search_nikkei_xtrend(days=3))
    all_articles.extend(search_web_articles(days=2))

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    selected = filter_and_rank_articles(all_articles, sent_hashes, learning_data)
    print(f"Selected {len(selected)} articles")

    if len(selected) == 0:
        msg = f"ğŸ“° *ä»Šæ—¥ã®ãŠã™ã™ã‚è¨˜äº‹* ({datetime.now().strftime('%Y-%m-%d')})\n"
        msg += f"<@{SLACK_USER_ID}>\n\nä»Šæ—¥ã¯æ–°ã—ã„è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        post_to_slack(msg)
        return

    # ãƒ˜ãƒƒãƒ€ãƒ¼æŠ•ç¨¿
    header = f"ğŸ“° *ä»Šæ—¥ã®ãŠã™ã™ã‚è¨˜äº‹* ({datetime.now().strftime('%Y-%m-%d')})\n"
    header += f"<@{SLACK_USER_ID}>\n\nè‰¯ã‹ã£ãŸè¨˜äº‹ã«ã¯ğŸ‘ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã¤ã‘ã¦ãã ã•ã„ï¼"
    post_to_slack(header)
    print("âœ… Posted header")

    time.sleep(1)

    # å€‹åˆ¥è¨˜äº‹æŠ•ç¨¿
    emojis = ['1ï¸âƒ£', '2ï¸âƒ£', '3ï¸âƒ£', '4ï¸âƒ£', '5ï¸âƒ£']
    for idx, article in enumerate(selected):
        emoji = emojis[idx]
        lang_flag = 'ğŸ‡¯ğŸ‡µ' if article.lang == 'ja' else 'ğŸ‡ºğŸ‡¸'

        msg = f"{emoji} *{article.title}* {lang_flag}\n"
        msg += f"ğŸ”— {article.url}\n"
        msg += f"ğŸ“ {article.summary}\n"
        msg += f"ğŸ·ï¸ {' '.join(['#' + t for t in article.tags])} | ğŸ“° {article.source}"

        ts = post_to_slack(msg, article)

        if ts:
            print(f"âœ… Posted article {idx+1}")
            time.sleep(0.3)
            add_reaction(ts, 'thumbsup')
            time.sleep(0.2)
            add_reaction(ts, 'thumbsdown')

        time.sleep(0.5)

    # æ—¢èª­ä¿å­˜
    save_sent_articles([a.hash for a in selected])

    print("=== Finished ===")


if __name__ == '__main__':
    main()
