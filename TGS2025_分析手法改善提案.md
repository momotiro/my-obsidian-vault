# TGS2025 UGC分析 - 分析手法の改善提案

## 📚 世界標準のフレームワーク調査結果

### 1. **学術的フレームワーク（Springer Nature, 2024-2025）**

#### **de Haan et al. (2024) の3ステップアプローチ**
マーケティングマネージャー向けの構造化された方法論：
1. データ収集技術の選択
2. 機械学習・深層学習手法の適用
3. 実用的なユースケースへの展開

#### **顧客ジャーニーフレームワーク（342論文のメタ分析）**
6つのUGC次元で分析：
- UGC特性
- 商品特性
- 投稿者特性
- 消費者特性
- インタラクション特性
- その他の特性

これを3段階の顧客ジャーニー（購入前・購入中・購入後）で評価

---

## 🔍 今回の分析の課題点

### **課題1: 投稿者セグメンテーションの不足**
- ✅ 実施したこと: ブース別の集計
- ❌ 不足していたこと:
  - 投稿者のフォロワー数による影響力分析（インフルエンサー vs 一般ユーザー）
  - リピート投稿者 vs 初回投稿者の分析
  - 投稿者の属性（コスプレイヤー、ゲーマー、業界関係者など）

**改善案:**
```python
# フォロワー数による投稿者分類
def classify_user_influence(row):
    # 筆者のフォロワー数などから推定
    # Micro: < 10K, Mid: 10K-100K, Macro: 100K-1M, Mega: > 1M
    pass

# リピート投稿者の特定
user_post_counts = df.groupby('筆者ハンドルネーム').size()
repeat_users = user_post_counts[user_post_counts > 1]
```

---

### **課題2: 時系列分析の不足**
- ✅ 実施したこと: 日別投稿数の集計
- ❌ 不足していたこと:
  - 時間帯別の投稿パターン（開場直後、昼、夕方など）
  - 投稿の時系列トレンド（初日からの推移）
  - イベント前・期間中・イベント後の拡散パターン

**改善案:**
```python
# 時間帯分析
df['hour'] = pd.to_datetime(df['時間']).dt.hour
df['day_of_event'] = (df['日付'] - event_start_date).dt.days

# ブース別の時系列推移
booth_timeline = df.groupby(['primary_booth', 'day_of_event']).size()
```

---

### **課題3: センチメント分析の深度不足**
- ✅ 実施したこと: センチメント（positive/neutral/negative）の集計
- ❌ 不足していたこと:
  - **アスペクトベース・センチメント分析**（何に対してポジティブ/ネガティブか）
  - 感情の細分化（喜び、興奮、失望、怒りなど）
  - 本文テキストのNLP分析（頻出語、トピックモデリング）

**改善案（最新の手法）:**
```python
# LLMベース（GPT/BERT）のセンチメント分析
from transformers import pipeline

sentiment_analyzer = pipeline(
    "sentiment-analysis",
    model="cardiffnlp/twitter-roberta-base-sentiment-multilingual"
)

# アスペクトベース分析
aspects = ['グッズ', '試遊', 'ステージ', 'スタッフ', '待ち時間']
for aspect in aspects:
    aspect_posts = df[df['該当文章'].str.contains(aspect)]
    aspect_sentiment = sentiment_analyzer(aspect_posts['該当文章'].tolist())
```

---

### **課題4: ネットワーク分析の欠如**
- ✅ 実施したこと: エンゲージメント指標の集計
- ❌ 不足していたこと:
  - リツイート・引用のネットワーク分析
  - インフルエンサーからの拡散経路
  - クラスター分析（どのコミュニティが反応したか）

**改善案:**
```python
import networkx as nx

# リツイート/引用ネットワークの構築
G = nx.DiGraph()
for _, row in df.iterrows():
    if row['Content Type'] == 'Quote':
        # 引用元と引用先のネットワーク
        G.add_edge(row['筆者'], row['引用元'])

# 中心性分析（最も影響力のあるノード）
centrality = nx.betweenness_centrality(G)
```

---

### **課題5: 因果推論・アトリビューション分析の不足**
- ✅ 実施したこと: 相関分析（投稿数とエンゲージメント率の相関）
- ❌ 不足していたこと:
  - **因果関係の推定**（施策Aが投稿数Bを増やしたのか、逆か）
  - マーケティング・ミックス・モデリング（MMM）
  - A/Bテスト的な比較（施策あり vs なし）

**改善案（Marketing Mix Modeling）:**
```python
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# 重回帰分析で各施策の寄与度を推定
X = booth_stats[[
    'グッズ割合', 'キャンペーン割合', '体験型割合',
    'フォトスポット割合', '画像投稿率', 'ハッシュタグ数平均'
]]
y = booth_stats['投稿数']

model = sm.OLS(y, sm.add_constant(X)).fit()
print(model.summary())

# 各施策の限界効果
print(model.params)
```

---

### **課題6: コンテンツ分析の定性的深度不足**
- ✅ 実施したこと: キーワードベースの施策分類
- ❌ 不足していたこと:
  - **トピックモデリング**（LDA, BERTopic）
  - 画像分析（画像の色調、構図、被写体の種類）
  - ビデオ分析（動画の長さ、内容タイプ）

**改善案（BERTopicによるトピックモデリング）:**
```python
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# 日本語対応の埋め込みモデル
embedding_model = SentenceTransformer('intfloat/multilingual-e5-large')

# トピック抽出
topic_model = BERTopic(
    embedding_model=embedding_model,
    language='japanese'
)

topics, probs = topic_model.fit_transform(df_ugc['該当文章'])

# トピック別の投稿数とエンゲージメント
topic_analysis = df_ugc.groupby('topic').agg({
    'ドキュメントID': 'count',
    'いいね数': 'mean',
    'リーチ': 'sum'
})
```

---

### **課題7: 外部要因・文脈の考慮不足**
- ✅ 実施したこと: ブース内の施策分析
- ❌ 不足していたこと:
  - 天候、気温などの外部要因
  - 競合ブースとの比較（隣接ブースの影響）
  - メディア露出（プレスリリース、ニュース記事）
  - IPの知名度（既存ファンベースの規模）

**改善案:**
```python
# 外部データの統合
booth_stats['ip_popularity'] = booth_stats.index.map(ip_fanbase_dict)
booth_stats['media_mentions'] = booth_stats.index.map(media_count_dict)
booth_stats['booth_size'] = booth_stats.index.map(booth_size_dict)

# 多変量回帰で外部要因を制御
X_extended = booth_stats[[
    'グッズ割合', 'キャンペーン割合', '体験型割合',
    'ip_popularity', 'media_mentions', 'booth_size'
]]
```

---

### **課題8: 予測モデルの不在**
- ✅ 実施したこと: 過去データの記述統計
- ❌ 不足していたこと:
  - **予測モデル**（TGS2026の投稿数予測）
  - シミュレーション（「グッズ予算を2倍にしたら？」）
  - 最適化（予算制約下での最適な施策配分）

**改善案（予測モデル）:**
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# 訓練データとテストデータに分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# ランダムフォレストで投稿数を予測
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 特徴量の重要度
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

# TGS2026のシミュレーション
new_booth_plan = pd.DataFrame({
    'グッズ割合': [0.3],
    'キャンペーン割合': [0.4],
    '体験型割合': [0.2],
    'フォトスポット割合': [0.1],
    '画像投稿率': [0.85],
    'ハッシュタグ数平均': [4.0]
})
predicted_posts = rf_model.predict(new_booth_plan)
print(f"予測投稿数: {predicted_posts[0]:.0f}")
```

---

## 🎯 推奨される分析フレームワーク（統合版）

### **Phase 0: データ収集・前処理**
```
✓ データクレンジング
✓ 投稿者セグメンテーション（影響力・属性）
✓ 時系列データの整備
✓ 外部データの統合（天候、IP人気度、メディア露出など）
```

### **Phase 1: 記述統計（Descriptive Analytics）**
```
✓ 基本統計量
✓ ブース別パフォーマンス
✓ 時系列トレンド
✓ セグメント別分析
```

### **Phase 2: 探索的分析（Exploratory Analytics）**
```
✓ クラスター分析（類似ブースのグルーピング）
✓ ネットワーク分析（拡散経路の可視化）
✓ トピックモデリング（投稿内容の自動分類）
✓ アスペクトベース・センチメント分析
```

### **Phase 3: 因果推論（Causal Inference）**
```
✓ 重回帰分析（多変量制御）
✓ マーケティング・ミックス・モデリング（MMM）
✓ 傾向スコアマッチング（施策あり vs なしの公正な比較）
✓ Difference-in-Differences（DID）分析
```

### **Phase 4: 予測モデリング（Predictive Analytics）**
```
✓ 機械学習モデル（ランダムフォレスト、XGBoost）
✓ 時系列予測（ARIMA, Prophet）
✓ 深層学習（LSTMなど）
```

### **Phase 5: 最適化（Prescriptive Analytics）**
```
✓ シミュレーション（施策変更の影響予測）
✓ 予算配分最適化（線形計画法）
✓ A/Bテスト設計（TGS2026での実験計画）
```

---

## 📊 具体的な改善実装例

### **改善1: 投稿者影響力分析**

```python
# 投稿者のフォロワー数推定（リーチから逆算）
df_ugc['estimated_followers'] = df_ugc['リーチ'] / df_ugc['いいね数'].fillna(1)

# 影響力レベルの分類
def classify_influence(followers):
    if followers < 1000: return 'nano'
    elif followers < 10000: return 'micro'
    elif followers < 100000: return 'mid'
    elif followers < 1000000: return 'macro'
    else: return 'mega'

df_ugc['influencer_tier'] = df_ugc['estimated_followers'].apply(classify_influence)

# ブース別の影響力分析
booth_influencer_mix = df_ugc.groupby(['primary_booth', 'influencer_tier']).size().unstack(fill_value=0)
```

### **改善2: BERTopicによるトピック分析**

```python
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# 日本語対応モデル
embedding_model = SentenceTransformer('intfloat/multilingual-e5-large')
topic_model = BERTopic(embedding_model=embedding_model, language='japanese', min_topic_size=10)

# 投稿内容からトピック抽出
docs = df_ugc['該当文章'].fillna('').tolist()
topics, probs = topic_model.fit_transform(docs)

# トピックラベルの取得
topic_labels = topic_model.get_topic_info()

# トピック別のエンゲージメント分析
df_ugc['topic'] = topics
topic_engagement = df_ugc.groupby('topic').agg({
    'ドキュメントID': 'count',
    'いいね数': ['mean', 'sum'],
    'リーチ': ['mean', 'sum']
})
```

### **改善3: Marketing Mix Modeling（MMM）**

```python
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler

# 説明変数の標準化
scaler = StandardScaler()

features = [
    'グッズ投稿割合', 'キャンペーン投稿割合', '体験型投稿割合',
    'フォトスポット投稿割合', '画像投稿率', 'ハッシュタグ平均',
    'インフルエンサー投稿割合', 'ステージ投稿割合'
]

X = booth_stats[features]
X_scaled = scaler.fit_transform(X)
X_scaled = sm.add_constant(X_scaled)

y = booth_stats['投稿数']

# 重回帰分析
model = sm.OLS(y, X_scaled).fit()
print(model.summary())

# 各施策の弾力性（1%増やしたときの投稿数への影響）
elasticity = model.params[1:] * X.mean() / y.mean()
elasticity_df = pd.DataFrame({
    'feature': features,
    'coefficient': model.params[1:],
    'elasticity': elasticity
}).sort_values('elasticity', ascending=False)

print("\n【施策別の弾力性】")
print(elasticity_df)
```

### **改善4: ネットワーク分析**

```python
import networkx as nx
import matplotlib.pyplot as plt

# リプライ・引用のネットワーク構築
G = nx.DiGraph()

for _, row in df_ugc[df_ugc['Content Type'].isin(['Reply', 'Quote'])].iterrows():
    author = row['筆者ハンドルネーム']
    # URLから引用元/リプライ先を抽出（簡略版）
    if pd.notna(row['URL']):
        # 実際にはURLをパースして引用元を特定
        pass

# 中心性分析
degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)

# 最も影響力のあるアカウント
top_influencers = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
```

### **改善5: 予測モデル（XGBoost）**

```python
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score

# 特徴量エンジニアリング
X = booth_stats[[
    'グッズ投稿割合', 'キャンペーン投稿割合', '体験型投稿割合',
    'フォトスポット投稿割合', '画像投稿率', 'ハッシュタグ平均',
    'インフルエンサー投稿割合', 'ステージ投稿割合'
]]
y = booth_stats['投稿数']

# XGBoostモデル
xgb_model = XGBRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)

# クロスバリデーション
cv_scores = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')
print(f"CV R² Score: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")

# モデル訓練
xgb_model.fit(X, y)

# 特徴量の重要度
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

# TGS2026の予測シミュレーション
scenarios = pd.DataFrame({
    'グッズ投稿割合': [0.2, 0.3, 0.4],
    'キャンペーン投稿割合': [0.3, 0.3, 0.3],
    '体験型投稿割合': [0.1, 0.2, 0.2],
    'フォトスポット投稿割合': [0.1, 0.1, 0.15],
    '画像投稿率': [0.75, 0.80, 0.85],
    'ハッシュタグ平均': [3.0, 3.5, 4.0],
    'インフルエンサー投稿割合': [0.05, 0.10, 0.15],
    'ステージ投稿割合': [0.05, 0.05, 0.10]
})

predictions = xgb_model.predict(scenarios)
scenarios['予測投稿数'] = predictions
print("\n【TGS2026シミュレーション】")
print(scenarios)
```

---

## 🌟 世界標準の分析手法まとめ

| 分析レベル | 手法 | 今回の実施 | 推奨度 |
|-----------|------|-----------|-------|
| **記述統計** | 基本集計、可視化 | ✅ 実施 | ⭐⭐⭐⭐⭐ |
| **セグメンテーション** | クラスター分析、RFM分析 | ⚠️ 部分的 | ⭐⭐⭐⭐⭐ |
| **センチメント分析** | BERT/GPT、アスペクトベース | ⚠️ 基本のみ | ⭐⭐⭐⭐⭐ |
| **トピックモデリング** | LDA, BERTopic | ❌ 未実施 | ⭐⭐⭐⭐ |
| **ネットワーク分析** | グラフ理論、中心性分析 | ❌ 未実施 | ⭐⭐⭐⭐ |
| **因果推論** | 重回帰、MMM、DID | ⚠️ 相関のみ | ⭐⭐⭐⭐⭐ |
| **予測モデル** | ML/DL（RF, XGBoost, LSTM） | ❌ 未実施 | ⭐⭐⭐⭐⭐ |
| **最適化** | シミュレーション、線形計画法 | ❌ 未実施 | ⭐⭐⭐⭐ |
| **画像/動画分析** | CV、オブジェクト検出 | ❌ 未実施 | ⭐⭐⭐ |
| **時系列分析** | ARIMA, Prophet | ⚠️ 基本のみ | ⭐⭐⭐⭐ |

---

## 📝 次回分析への推奨事項

### **優先度 HIGH（必須）**
1. ✅ **投稿者セグメンテーション**（影響力分析）
2. ✅ **因果推論・MMM**（施策の真の効果を測定）
3. ✅ **予測モデル構築**（TGS2026への実用的提言）
4. ✅ **アスペクトベース・センチメント分析**（何が評価されたか）

### **優先度 MEDIUM（推奨）**
5. ✅ **トピックモデリング**（自動的な投稿内容分類）
6. ✅ **ネットワーク分析**（拡散経路の可視化）
7. ✅ **時系列詳細分析**（時間帯・曜日別パターン）

### **優先度 LOW（余力があれば）**
8. ◯ **画像分析**（画像の色調、構図分析）
9. ◯ **外部データ統合**（天候、メディア露出など）
10. ◯ **最適化モデル**（予算配分の最適化）

---

## 🎓 参考文献・学術的根拠

1. **de Haan et al. (2024)** - "Collecting and Analyzing User-Generated Content for Decision Support in Marketing Management", *Schmalenbach Journal of Business Research*, Springer Nature
   - 3ステップUGC分析フレームワーク

2. **Springer Nature (2025)** - "Unraveling the Influence: Exploring the Role of User Generated Content Along the Customer Journey"
   - 342論文のメタ分析、6次元×3段階の顧客ジャーニーフレームワーク

3. **LLMベース分析** - "User needs insights from UGC based on large language model" (2025), *ScienceDirect*
   - GPT/BERTを用いたセンチメント分析とIPA-Kanoモデル

4. **Marketing Mix Modeling** - "Marketing Mix Modeling vs. Attribution: Choosing the Right Approach" (2024)
   - MMM vs アトリビューション分析の統合フレームワーク

5. **NLP Best Practices** - "Recent advancements and challenges of NLP-based sentiment analysis" (2024), *ScienceDirect*
   - BERT, RoBERTa, GPTを用いた最新のセンチメント分析手法

6. **Event Marketing ROI** - "7 Key Metrics to Measure UGC ROI in 2024"
   - リーチ、エンゲージメント、コンバージョン、センチメント、コンテンツ品質、インフルエンサー影響、CLVの7指標

---

## ✅ 結論

今回の分析は**記述統計レベル**では十分な品質でしたが、**因果推論・予測・最適化**のレベルに到達していませんでした。

次回は以下を実装することで、**世界標準のUGC分析**に到達できます：

```
Phase 0: データ拡充（投稿者属性、時系列詳細、外部データ）
  ↓
Phase 1-2: 記述統計 + 探索的分析（トピックモデリング、ネットワーク分析）
  ↓
Phase 3: 因果推論（MMM、重回帰、傾向スコアマッチング）
  ↓
Phase 4: 予測モデル（XGBoost、LSTMなど）
  ↓
Phase 5: 最適化・シミュレーション（TGS2026への具体的施策提案）
```

これにより、「何が起きたか」だけでなく、**「なぜ起きたか」「次に何をすべきか」「どれくらいの効果が見込めるか」**まで答えられる分析になります。
